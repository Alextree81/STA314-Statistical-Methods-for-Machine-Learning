{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_ChUFBpzd0a"
   },
   "source": [
    "This is a notebook for HW1. Be careful that variable values persist, so you may end up getting strange errors that are only solved when you restart the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0eCdzLUUzHVC"
   },
   "outputs": [],
   "source": [
    "# These are imports and you do not need to modify these.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import random\n",
    "import math\n",
    "import urllib.request\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(3142021)\n",
    "random.seed(3142021)\n",
    "\n",
    "def load_data():\n",
    "    # Load the data\n",
    "    with urllib.request.urlopen(\"https://www.cs.toronto.edu/~cmaddis/courses/sta314_f21/data/clean_fake.txt\") as f:\n",
    "        fake = [l.decode(\"utf-8\").strip() for l in f]\n",
    "    with urllib.request.urlopen(\"https://www.cs.toronto.edu/~cmaddis/courses/sta314_f21/data/clean_real.txt\") as f:\n",
    "        real = [l.decode(\"utf-8\").strip() for l in f]\n",
    "\n",
    "    # Each element is a string, corresponding to a headline\n",
    "    data = np.array(real + fake)\n",
    "    labels = np.array([0]*len(real) + [1]*len(fake))\n",
    "    return data, labels\n",
    "\n",
    "def main():\n",
    "    data, labels = load_data()\n",
    "    train_X, train_Y, val_X, val_Y, test_X, test_Y = process_data(data, labels)\n",
    "\n",
    "    best_model, best_k = select_knn_model(train_X, val_X, train_Y, val_Y)\n",
    "    test_accuracy = best_model.score(test_X, test_Y)\n",
    "    print(\"Selected K: {}\".format(best_k))\n",
    "    print(\"Test Acc: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdwIY8YfzqNo"
   },
   "source": [
    "You need to fill in the following two functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XfuC9nQPzQy1"
   },
   "outputs": [],
   "source": [
    "def process_data(data, labels):\n",
    "    \"\"\"\n",
    "    Preprocess a dataset of strings into vector representations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: numpy array\n",
    "        An array of N strings.\n",
    "    labels: numpy array\n",
    "        An array of N integer labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_X: numpy array\n",
    "        Array with shape (N, D) of N inputs.\n",
    "    train_Y:\n",
    "        Array with shape (N,) of N labels.\n",
    "    val_X:\n",
    "        Array with shape (M, D) of M inputs.\n",
    "    val_Y:\n",
    "        Array with shape (M,) of M labels.\n",
    "    test_X:\n",
    "        Array with shape (M, D) of M inputs.\n",
    "    test_Y:\n",
    "        Array with shape (M,) of M labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the dataset of string into train, validation, and test \n",
    "    # Use a 70/15/15 split\n",
    "    # train_test_split shuffles the data before splitting it \n",
    "    # Stratify keeps the proportion of labels the same in each split\n",
    "\n",
    "    # -- WRITE THE SPLITTING CODE HERE -- \n",
    "\n",
    "    # Preprocess each dataset of strings into a dataset of feature vectors\n",
    "    # using the CountVectorizer function. \n",
    "    # Note, fit the Vectorizer using the training set only, and then\n",
    "    # transform the validation and test sets.\n",
    "\n",
    "    # -- WRITE THE PROCESSING CODE HERE -- \n",
    "\n",
    "    # Return the training, validation, and test set inputs and labels\n",
    "\n",
    "    # -- RETURN THE ARRAYS HERE -- \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2E0tFcZzbu6"
   },
   "outputs": [],
   "source": [
    "def select_knn_model(train_X, val_X, train_Y, val_Y):\n",
    "    \"\"\"\n",
    "    Test k in {1, ..., 20} and return the a k-NN model\n",
    "    fitted to the training set with the best validation loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_X: numpy array\n",
    "        Array with shape (N, D) of N inputs.\n",
    "    train_Y: numpy array\n",
    "        Array with shape (M, D) of M inputs.\n",
    "    train_Y: numpy array\n",
    "        Array with shape (N,) of N labels.\n",
    "    val_Y: numpy array\n",
    "        Array with shape (M,) of M labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_model : KNeighborsClassifier\n",
    "        The best k-NN classifier fit on the training data \n",
    "    and selected according to validation loss.\n",
    "    best_k : int\n",
    "        The best k value according to validation loss.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BY13RYNLztWg"
   },
   "source": [
    "Run the next cell to get the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xg8Fb725zv_Q"
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
