{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"STA314 Homework 3.\n",
    "\n",
    "Copyright and Usage Information\n",
    "===============================\n",
    "\n",
    "This file is provided solely for the personal and private use of students\n",
    "taking STA314 at the University of Toronto St. George campus. All forms of\n",
    "distribution of this code, whether as given or with any changes, are\n",
    "expressly prohibited.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may find these helper functions useful\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\" Computes the element wise logistic sigmoid of x.\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def load_train():\n",
    "    \"\"\" Loads training data for digits_train.\n",
    "    \"\"\"\n",
    "    response = requests.get(\"https://www.cs.toronto.edu/~cmaddis/courses/sta314_f21/data/digits.npz\")\n",
    "    response.raise_for_status()\n",
    "    data = np.load(io.BytesIO(response.content))\n",
    "    train_inputs = np.hstack((data[\"train2\"], data[\"train3\"]))\n",
    "    train_targets = np.hstack((np.zeros((1, data[\"train2\"].shape[1])), np.ones((1, data[\"train3\"].shape[1]))))\n",
    "    return train_inputs.T, train_targets.T\n",
    "\n",
    "\n",
    "def load_train_small():\n",
    "    \"\"\" Loads training data for digits_train_small.\n",
    "    \"\"\"\n",
    "    response = requests.get(\"https://www.cs.toronto.edu/~cmaddis/courses/sta314_f21/data/digits.npz\")\n",
    "    response.raise_for_status()\n",
    "    data = np.load(io.BytesIO(response.content))\n",
    "    train_inputs = np.hstack((data[\"train2\"][:, :2], data[\"train3\"][:, :2]))\n",
    "    train_targets = np.hstack((np.zeros((1, 2)), np.ones((1, 2))))\n",
    "    return train_inputs.T, train_targets.T\n",
    "\n",
    "\n",
    "def load_valid():\n",
    "    \"\"\" Loads validation data.\n",
    "    \"\"\"\n",
    "    response = requests.get(\"https://www.cs.toronto.edu/~cmaddis/courses/sta314_f21/data/digits.npz\")\n",
    "    response.raise_for_status()\n",
    "    data = np.load(io.BytesIO(response.content))\n",
    "    valid_inputs = np.hstack((data[\"valid2\"], data[\"valid3\"]))\n",
    "    valid_targets = np.hstack((np.zeros((1, data[\"valid2\"].shape[1])), np.ones((1, data[\"valid3\"].shape[1]))))\n",
    "    return valid_inputs.T, valid_targets.T\n",
    "\n",
    "\n",
    "def load_test():\n",
    "    \"\"\" Loads validation data.\n",
    "    \"\"\"\n",
    "    response = requests.get(\"https://www.cs.toronto.edu/~cmaddis/courses/sta314_f21/data/digits.npz\")\n",
    "    response.raise_for_status()\n",
    "    data = np.load(io.BytesIO(response.content))\n",
    "    test_inputs = np.hstack((data[\"test2\"], data[\"test3\"]))\n",
    "    test_targets = np.hstack((np.zeros((1, data[\"test2\"].shape[1])), np.ones((1, data[\"test3\"].shape[1]))))\n",
    "    return test_inputs.T, test_targets.T\n",
    "\n",
    "\n",
    "def plot_digits(digit_array):\n",
    "    \"\"\" Visualizes each example in digit_array.\n",
    "    :param digit_array: N x D array of pixel intensities.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    CLASS_EXAMPLES_PER_PANE = 5\n",
    "\n",
    "    # assume two evenly split classes\n",
    "    examples_per_class = int(digit_array.shape[0] / 2)\n",
    "    num_panes = int(np.ceil(float(examples_per_class) / CLASS_EXAMPLES_PER_PANE))\n",
    "\n",
    "    for pane in range(num_panes):\n",
    "        print(\"Displaying pane {}/{}\".format(pane + 1, num_panes))\n",
    "\n",
    "        top_start = pane * CLASS_EXAMPLES_PER_PANE\n",
    "        top_end = min((pane + 1) * CLASS_EXAMPLES_PER_PANE, examples_per_class)\n",
    "        top_pane_digits = extract_digits(digit_array, top_start, top_end)\n",
    "\n",
    "        bottom_start = top_start + examples_per_class\n",
    "        bottom_end = top_end + examples_per_class\n",
    "        bottom_pane_digits = extract_digits(digit_array, bottom_start, bottom_end)\n",
    "\n",
    "        show_pane(top_pane_digits, bottom_pane_digits)\n",
    "\n",
    "\n",
    "def extract_digits(digit_array, start_index, end_index):\n",
    "    \"\"\" Returns a list of 16 x 16 pixel intensity arrays starting\n",
    "    at start_index and ending at end_index.\n",
    "    \"\"\"\n",
    "    digits = []\n",
    "    for index in range(start_index, end_index):\n",
    "        digits.append(extract_digit_pixels(digit_array, index))\n",
    "    return digits\n",
    "\n",
    "\n",
    "def extract_digit_pixels(digit_array, index):\n",
    "    \"\"\" Extracts the 16 x 16 pixel intensity array at the specified index.\n",
    "    \"\"\"\n",
    "    return digit_array[index].reshape(16, 16).T\n",
    "\n",
    "\n",
    "def show_pane(top_digits, bottom_digits):\n",
    "    \"\"\" Displays two rows of digits on the screen.\n",
    "    \"\"\"\n",
    "    all_digits = top_digits + bottom_digits\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=int(len(all_digits) / 2))\n",
    "    for axis, digit in zip(axes.reshape(-1), all_digits):\n",
    "        axis.imshow(digit, interpolation=\"nearest\", cmap=plt.gray())\n",
    "        axis.set_xticklabels([])\n",
    "        axis.set_yticklabels([])\n",
    "        axis.axis(\"off\")\n",
    "    # fig.subplots_adjust(wspace=0,\n",
    "    #                     hspace=0)\n",
    "    plt.tight_layout(h_pad=-7)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_images(images, filename):\n",
    "    fig = plt.figure(1)\n",
    "    fig.clf()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plot_digits(images)\n",
    "    fig.patch.set_visible(False)\n",
    "    ax.patch.set_visible(False)\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_predict(weights, data):\n",
    "    \"\"\" Compute the probabilities predicted by the logistic classifier.\n",
    "\n",
    "    Note: N is the number of examples\n",
    "          D is the number of features per example\n",
    "\n",
    "    :param weights: A vector of weights with dimension (D + 1) x 1, where\n",
    "    the last element corresponds to the bias (intercept).\n",
    "    :param data: A matrix with dimension N x D, where each row corresponds to\n",
    "    one data point.\n",
    "    :return: A vector of probabilities with dimension N x 1, which is the output\n",
    "    to the classifier.\n",
    "    \"\"\"\n",
    "    #####################################################################\n",
    "    # TODO:                                                             #\n",
    "    #####################################################################\n",
    "    y = None\n",
    "    #####################################################################\n",
    "    #                       END OF YOUR CODE                            #\n",
    "    #####################################################################\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(targets, y):\n",
    "    \"\"\" Compute evaluation metrics.\n",
    "\n",
    "    Note: N is the number of examples\n",
    "          D is the number of features per example\n",
    "\n",
    "    :param targets: A vector of targets with dimension N x 1.\n",
    "    :param y: A vector of probabilities with dimension N x 1.\n",
    "    :return: A tuple (ce, frac_correct)\n",
    "        WHERE\n",
    "        ce: (float) Averaged cross entropy\n",
    "        frac_correct: (float) Fraction of inputs classified correctly\n",
    "    \"\"\"\n",
    "    #####################################################################\n",
    "    # TODO:                                                             #\n",
    "    #####################################################################\n",
    "    ce = None\n",
    "    frac_correct = None\n",
    "    #####################################################################\n",
    "    #                       END OF YOUR CODE                            #\n",
    "    #####################################################################\n",
    "    return ce, frac_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(weights, data, targets, hyperparameters):\n",
    "    \"\"\" Calculate the cost of penalized logistic regression and its derivatives\n",
    "    with respect to weights. Also return the predictions.\n",
    "\n",
    "    Note: N is the number of examples\n",
    "          D is the number of features per example\n",
    "\n",
    "    :param weights: A vector of weights with dimension (D + 1) x 1, where\n",
    "    the last element corresponds to the bias (intercept).\n",
    "    :param data: A matrix with dimension N x D, where each row corresponds to\n",
    "    one data point.\n",
    "    :param targets: A vector of targets with dimension N x 1.\n",
    "    :param hyperparameters: The hyperparameter dictionary.\n",
    "    :returns: A tuple (f, df, y)\n",
    "        WHERE\n",
    "        f: The average of the loss over all data points, plus a penalty term.\n",
    "           This is the objective that we want to minimize.\n",
    "        df: (D+1) x 1 vector of derivative of f w.r.t. weights.\n",
    "        y: N x 1 vector of probabilities.\n",
    "    \"\"\"\n",
    "    y = logistic_predict(weights, data)\n",
    "    lambd = hyperparameters[\"weight_regularization\"]\n",
    "    #####################################################################\n",
    "    # TODO:                                                             #\n",
    "    #####################################################################\n",
    "    f = None\n",
    "    df = None\n",
    "    #####################################################################\n",
    "    #                       END OF YOUR CODE                            #\n",
    "    #####################################################################\n",
    "    return f, df, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logistic_regression():\n",
    "    # Load all necessary datasets:\n",
    "    x_train, y_train = load_train()\n",
    "    # If you would like to use digits_train_small, please uncomment this line:\n",
    "    # x_train, y_train = load_train_small()\n",
    "    x_valid, y_valid = load_valid()\n",
    "    x_test, y_test = load_test()\n",
    "\n",
    "    n, d = x_train.shape\n",
    "\n",
    "    #####################################################################\n",
    "    # TODO:                                                             #\n",
    "    # Set the hyperparameters for the learning rate, the number         #\n",
    "    # of iterations                                                     #\n",
    "    #####################################################################\n",
    "    hyperparameters = {\n",
    "        \"learning_rate\": None,\n",
    "        \"weight_regularization\": 0.,\n",
    "        \"num_iterations\": None\n",
    "    }\n",
    "    #####################################################################\n",
    "    #                       END OF YOUR CODE                            #\n",
    "    #####################################################################\n",
    "\n",
    "    # Begin learning with gradient descent\n",
    "    #####################################################################\n",
    "    # TODO:                                                             #\n",
    "    # Modify this section to perform gradient descent, create plots,    #\n",
    "    # compute test error, etc ...                                       #\n",
    "    #####################################################################\n",
    "    weights = np.zeros((d + 1, 1))\n",
    "    for t in range(hyperparameters[\"num_iterations\"]):\n",
    "        pass\n",
    "    #####################################################################\n",
    "    #                       END OF YOUR CODE                            #\n",
    "    #####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2b1dd5677a53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-fcc44a5e18ed>\u001b[0m in \u001b[0;36mrun_logistic_regression\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#####################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_iterations\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m#####################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "run_logistic_regression()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
